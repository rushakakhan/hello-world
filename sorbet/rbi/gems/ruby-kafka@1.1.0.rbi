# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `ruby-kafka` gem.
# Please instead update this file by running `tapioca generate`.

# typed: true

module Kafka
  class << self
    def new(seed_brokers = T.unsafe(nil), **options); end
  end
end

class Kafka::AsyncProducer
  def initialize(sync_producer:, instrumenter:, logger:, max_queue_size: T.unsafe(nil), delivery_threshold: T.unsafe(nil), delivery_interval: T.unsafe(nil), max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil)); end

  def deliver_messages; end
  def produce(value, topic:, **options); end
  def shutdown; end

  private

  def buffer_overflow(topic, message); end
  def ensure_threads_running!; end
end

Kafka::AsyncProducer::THREAD_MUTEX = T.let(T.unsafe(nil), Thread::Mutex)

class Kafka::AsyncProducer::Timer
  def initialize(interval:, queue:); end

  def run; end
end

class Kafka::AsyncProducer::Worker
  def initialize(queue:, producer:, delivery_threshold:, instrumenter:, logger:, max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil)); end

  def run; end

  private

  def deliver_messages; end
  def produce(*args); end
  def threshold_reached?; end
end

class Kafka::Broker
  def initialize(connection_builder:, host:, port:, logger:, node_id: T.unsafe(nil)); end

  def add_offsets_to_txn(**options); end
  def add_partitions_to_txn(**options); end
  def address_match?(host, port); end
  def alter_configs(**options); end
  def api_versions; end
  def commit_offsets(**options); end
  def connected?; end
  def create_partitions(**options); end
  def create_topics(**options); end
  def delete_topics(**options); end
  def describe_configs(**options); end
  def describe_groups(**options); end
  def disconnect; end
  def end_txn(**options); end
  def fetch_messages(**options); end
  def fetch_metadata(**options); end
  def fetch_offsets(**options); end
  def find_coordinator(**options); end
  def heartbeat(**options); end
  def init_producer_id(**options); end
  def join_group(**options); end
  def leave_group(**options); end
  def list_groups; end
  def list_offsets(**options); end
  def produce(**options); end
  def sync_group(**options); end
  def to_s; end
  def txn_offset_commit(**options); end

  private

  def connection; end
  def send_request(request); end
end

class Kafka::BrokerInfo
  def initialize(node_id:, host:, port:); end

  def host; end
  def node_id; end
  def port; end
  def to_s; end
end

class Kafka::BrokerNotAvailable < ::Kafka::ProtocolError
end

class Kafka::BrokerPool
  def initialize(connection_builder:, logger:); end

  def close; end
  def connect(host, port, node_id: T.unsafe(nil)); end
end

module Kafka::BrokerUri
  class << self
    def parse(str); end
  end
end

Kafka::BrokerUri::DEFAULT_PORT = T.let(T.unsafe(nil), Integer)

Kafka::BrokerUri::URI_SCHEMES = T.let(T.unsafe(nil), Array)

class Kafka::BufferOverflow < ::Kafka::Error
end

class Kafka::Client
  def initialize(seed_brokers:, client_id: T.unsafe(nil), logger: T.unsafe(nil), connect_timeout: T.unsafe(nil), socket_timeout: T.unsafe(nil), ssl_ca_cert_file_path: T.unsafe(nil), ssl_ca_cert: T.unsafe(nil), ssl_client_cert: T.unsafe(nil), ssl_client_cert_key: T.unsafe(nil), ssl_client_cert_key_password: T.unsafe(nil), ssl_client_cert_chain: T.unsafe(nil), sasl_gssapi_principal: T.unsafe(nil), sasl_gssapi_keytab: T.unsafe(nil), sasl_plain_authzid: T.unsafe(nil), sasl_plain_username: T.unsafe(nil), sasl_plain_password: T.unsafe(nil), sasl_scram_username: T.unsafe(nil), sasl_scram_password: T.unsafe(nil), sasl_scram_mechanism: T.unsafe(nil), sasl_over_ssl: T.unsafe(nil), ssl_ca_certs_from_system: T.unsafe(nil), sasl_oauth_token_provider: T.unsafe(nil), ssl_verify_hostname: T.unsafe(nil)); end

  def alter_configs(broker_id, configs = T.unsafe(nil)); end
  def alter_topic(name, configs = T.unsafe(nil)); end
  def apis; end
  def async_producer(delivery_interval: T.unsafe(nil), delivery_threshold: T.unsafe(nil), max_queue_size: T.unsafe(nil), max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil), **options); end
  def brokers; end
  def close; end
  def consumer(group_id:, session_timeout: T.unsafe(nil), rebalance_timeout: T.unsafe(nil), offset_commit_interval: T.unsafe(nil), offset_commit_threshold: T.unsafe(nil), heartbeat_interval: T.unsafe(nil), offset_retention_time: T.unsafe(nil), fetcher_max_queue_size: T.unsafe(nil), refresh_topic_interval: T.unsafe(nil)); end
  def controller_broker; end
  def create_partitions_for(name, num_partitions: T.unsafe(nil), timeout: T.unsafe(nil)); end
  def create_topic(name, num_partitions: T.unsafe(nil), replication_factor: T.unsafe(nil), timeout: T.unsafe(nil), config: T.unsafe(nil)); end
  def delete_topic(name, timeout: T.unsafe(nil)); end
  def deliver_message(value, topic:, key: T.unsafe(nil), headers: T.unsafe(nil), partition: T.unsafe(nil), partition_key: T.unsafe(nil), retries: T.unsafe(nil)); end
  def describe_configs(broker_id, configs = T.unsafe(nil)); end
  def describe_group(group_id); end
  def describe_topic(name, configs = T.unsafe(nil)); end
  def each_message(topic:, start_from_beginning: T.unsafe(nil), max_wait_time: T.unsafe(nil), min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), &block); end
  def fetch_group_offsets(group_id); end
  def fetch_messages(topic:, partition:, offset: T.unsafe(nil), max_wait_time: T.unsafe(nil), min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), retries: T.unsafe(nil)); end
  def groups; end
  def has_topic?(topic); end
  def last_offset_for(topic, partition); end
  def last_offsets_for(*topics); end
  def partitions_for(topic); end
  def producer(compression_codec: T.unsafe(nil), compression_threshold: T.unsafe(nil), ack_timeout: T.unsafe(nil), required_acks: T.unsafe(nil), max_retries: T.unsafe(nil), retry_backoff: T.unsafe(nil), max_buffer_size: T.unsafe(nil), max_buffer_bytesize: T.unsafe(nil), idempotent: T.unsafe(nil), transactional: T.unsafe(nil), transactional_id: T.unsafe(nil), transactional_timeout: T.unsafe(nil)); end
  def replica_count_for(topic); end
  def supports_api?(api_key, version = T.unsafe(nil)); end
  def topics; end

  private

  def initialize_cluster; end
  def normalize_seed_brokers(seed_brokers); end
end

class Kafka::Cluster
  def initialize(seed_brokers:, broker_pool:, logger:); end

  def add_target_topics(topics); end
  def alter_configs(broker_id, configs = T.unsafe(nil)); end
  def alter_topic(name, configs = T.unsafe(nil)); end
  def api_info(api_key); end
  def apis; end
  def clear_target_topics; end
  def cluster_info; end
  def create_partitions_for(name, num_partitions:, timeout:); end
  def create_topic(name, num_partitions:, replication_factor:, timeout:, config:); end
  def delete_topic(name, timeout:); end
  def describe_configs(broker_id, configs = T.unsafe(nil)); end
  def describe_group(group_id); end
  def describe_topic(name, configs = T.unsafe(nil)); end
  def disconnect; end
  def fetch_group_offsets(group_id); end
  def get_group_coordinator(group_id:); end
  def get_leader(topic, partition); end
  def get_transaction_coordinator(transactional_id:); end
  def list_groups; end
  def list_topics; end
  def mark_as_stale!; end
  def partitions_for(topic); end
  def refresh_metadata!; end
  def refresh_metadata_if_necessary!; end
  def resolve_offset(topic, partition, offset); end
  def resolve_offsets(topic, partitions, offset); end
  def supports_api?(api_key, version = T.unsafe(nil)); end
  def topics; end

  private

  def connect_to_broker(broker_id); end
  def controller_broker; end
  def fetch_cluster_info; end
  def get_coordinator(coordinator_type, coordinator_key); end
  def get_leader_id(topic, partition); end
  def random_broker; end
end

class Kafka::ClusterAuthorizationFailed < ::Kafka::ProtocolError
end

module Kafka::Compression
  class << self
    def codecs; end
    def find_codec(name); end
    def find_codec_by_id(codec_id); end
  end
end

Kafka::Compression::CODECS_BY_ID = T.let(T.unsafe(nil), Hash)

Kafka::Compression::CODECS_BY_NAME = T.let(T.unsafe(nil), Hash)

class Kafka::Compressor
  def initialize(instrumenter:, codec_name: T.unsafe(nil), threshold: T.unsafe(nil)); end

  def codec; end
  def compress(record_batch, offset: T.unsafe(nil)); end

  private

  def compress_message_set(message_set, offset); end
  def compress_record_batch(record_batch); end
end

class Kafka::ConcurrentTransactionError < ::Kafka::Error
end

class Kafka::Connection
  def initialize(host:, port:, client_id:, logger:, instrumenter:, connect_timeout: T.unsafe(nil), socket_timeout: T.unsafe(nil), ssl_context: T.unsafe(nil)); end

  def close; end
  def decoder; end
  def encoder; end
  def open?; end
  def send_request(request); end
  def to_s; end

  private

  def idle?; end
  def open; end
  def read_response(response_class, notification); end
  def wait_for_response(response_class, notification); end
  def write_request(request, notification); end
end

Kafka::Connection::CONNECT_TIMEOUT = T.let(T.unsafe(nil), Integer)

Kafka::Connection::IDLE_TIMEOUT = T.let(T.unsafe(nil), Integer)

Kafka::Connection::SOCKET_TIMEOUT = T.let(T.unsafe(nil), Integer)

class Kafka::ConnectionBuilder
  def initialize(client_id:, logger:, instrumenter:, connect_timeout:, socket_timeout:, ssl_context:, sasl_authenticator:); end

  def build_connection(host, port); end
end

class Kafka::ConnectionError < ::Kafka::Error
end

class Kafka::Consumer
  def initialize(cluster:, logger:, instrumenter:, group:, fetcher:, offset_manager:, session_timeout:, heartbeat:, refresh_topic_interval: T.unsafe(nil)); end

  def commit_offsets; end
  def each_batch(min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), max_wait_time: T.unsafe(nil), automatically_mark_as_processed: T.unsafe(nil)); end
  def each_message(min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), max_wait_time: T.unsafe(nil), automatically_mark_as_processed: T.unsafe(nil)); end
  def mark_message_as_processed(message); end
  def pause(topic, partition, timeout: T.unsafe(nil), max_timeout: T.unsafe(nil), exponential_backoff: T.unsafe(nil)); end
  def paused?(topic, partition); end
  def resume(topic, partition); end
  def seek(topic, partition, offset); end
  def send_heartbeat; end
  def send_heartbeat_if_necessary; end
  def stop; end
  def subscribe(topic_or_regex, default_offset: T.unsafe(nil), start_from_beginning: T.unsafe(nil), max_bytes_per_partition: T.unsafe(nil)); end
  def trigger_heartbeat; end
  def trigger_heartbeat!; end

  private

  def clear_current_offsets(excluding: T.unsafe(nil)); end
  def cluster_topics; end
  def consumer_loop; end
  def fetch_batches; end
  def join_group; end
  def make_final_offsets_commit!(attempts = T.unsafe(nil)); end
  def pause_for(topic, partition); end
  def refresh_topic_list_if_enabled; end
  def resume_paused_partitions!; end
  def running?; end
  def scan_for_subscribing; end
  def seek_to_next(topic, partition); end
  def shutting_down?; end
  def subscribe_to_regex(topic_regex, default_offset, start_from_beginning, max_bytes_per_partition); end
  def subscribe_to_topic(topic, default_offset, start_from_beginning, max_bytes_per_partition); end
end

class Kafka::ConsumerGroup
  def initialize(cluster:, logger:, group_id:, session_timeout:, rebalance_timeout:, retention_time:, instrumenter:); end

  def assigned_partitions; end
  def assigned_to?(topic, partition); end
  def commit_offsets(offsets); end
  def fetch_offsets; end
  def generation_id; end
  def group_id; end
  def heartbeat; end
  def join; end
  def leave; end
  def member?; end
  def subscribe(topic); end
  def subscribed_partitions; end
  def to_s; end

  private

  def coordinator; end
  def group_leader?; end
  def join_group; end
  def synchronize; end
end

class Kafka::CoordinatorLoadInProgress < ::Kafka::ProtocolError
end

class Kafka::CoordinatorNotAvailable < ::Kafka::ProtocolError
end

class Kafka::CorruptMessage < ::Kafka::ProtocolError
end

class Kafka::DecoratingInstrumenter
  def initialize(backend, extra_payload = T.unsafe(nil)); end

  def instrument(event_name, payload = T.unsafe(nil), &block); end
end

class Kafka::DeliveryFailed < ::Kafka::Error
  def initialize(message, failed_messages); end

  def failed_messages; end
end

class Kafka::DuplicateSequenceNumberError < ::Kafka::Error
end

class Kafka::Error < ::StandardError
end

class Kafka::FailedScramAuthentication < ::Kafka::SaslScramError
end

class Kafka::FetchError < ::Kafka::Error
end

class Kafka::FetchOperation
  def initialize(cluster:, logger:, min_bytes: T.unsafe(nil), max_bytes: T.unsafe(nil), max_wait_time: T.unsafe(nil)); end

  def execute; end
  def fetch_from_partition(topic, partition, offset: T.unsafe(nil), max_bytes: T.unsafe(nil)); end
end

class Kafka::FetchedBatch
  def initialize(topic:, partition:, highwater_mark_offset:, messages:, last_offset: T.unsafe(nil), leader_epoch: T.unsafe(nil)); end

  def empty?; end
  def first_offset; end
  def highwater_mark_offset; end
  def last_offset; end
  def leader_epoch; end
  def messages; end
  def messages=(_); end
  def offset_lag; end
  def partition; end
  def topic; end
  def unknown_last_offset?; end
end

class Kafka::FetchedBatchGenerator
  def initialize(topic, fetched_partition, offset, logger:); end

  def generate; end

  private

  def abort_marker?(record_batch); end
  def empty_fetched_batch; end
  def extract_messages; end
  def extract_records; end
end

Kafka::FetchedBatchGenerator::ABORTED_TRANSACTION_SIGNAL = T.let(T.unsafe(nil), String)

Kafka::FetchedBatchGenerator::COMMITTED_TRANSACTION_SIGNAL = T.let(T.unsafe(nil), String)

class Kafka::FetchedMessage
  def initialize(message:, topic:, partition:); end

  def create_time; end
  def headers; end
  def is_control_record; end
  def key; end
  def offset; end
  def partition; end
  def topic; end
  def value; end
end

class Kafka::FetchedOffsetResolver
  def initialize(logger:); end

  def resolve!(broker, topics); end

  private

  def filter_pending_topics(topics); end
end

class Kafka::Fetcher
  def initialize(cluster:, logger:, instrumenter:, max_queue_size:, group:); end

  def configure(min_bytes:, max_bytes:, max_wait_time:); end
  def data?; end
  def max_wait_time; end
  def poll; end
  def queue; end
  def reset; end
  def seek(topic, partition, offset); end
  def start; end
  def stop; end
  def subscribe(topic, max_bytes_per_partition:); end

  private

  def current_reset_counter; end
  def fetch_batches; end
  def handle_configure(min_bytes, max_bytes, max_wait_time); end
  def handle_reset; end
  def handle_seek(topic, partition, offset); end
  def handle_stop(*_); end
  def handle_subscribe(topic, max_bytes_per_partition); end
  def loop; end
  def step; end
end

class Kafka::GroupAuthorizationFailed < ::Kafka::ProtocolError
end

class Kafka::GzipCodec
  def codec_id; end
  def compress(data); end
  def decompress(data); end
  def load; end
  def produce_api_min_version; end
end

class Kafka::Heartbeat
  def initialize(group:, interval:, instrumenter:); end

  def trigger; end
  def trigger!; end
end

class Kafka::HeartbeatError < ::Kafka::Error
end

class Kafka::IdleConnection < ::Kafka::Error
end

class Kafka::IllegalGeneration < ::Kafka::ProtocolError
end

class Kafka::InconsistentGroupProtocol < ::Kafka::ProtocolError
end

class Kafka::Instrumenter
  def initialize(default_payload = T.unsafe(nil)); end

  def instrument(event_name, payload = T.unsafe(nil), &block); end
end

Kafka::Instrumenter::NAMESPACE = T.let(T.unsafe(nil), String)

class Kafka::InsufficientDataMessage < ::Kafka::Error
end

class Kafka::InvalidCommitOffsetSize < ::Kafka::ProtocolError
end

class Kafka::InvalidConfig < ::Kafka::ProtocolError
end

class Kafka::InvalidGroupId < ::Kafka::ProtocolError
end

class Kafka::InvalidMessageSize < ::Kafka::ProtocolError
end

class Kafka::InvalidPartitions < ::Kafka::ProtocolError
end

class Kafka::InvalidProducerEpochError < ::Kafka::Error
end

class Kafka::InvalidProducerIDMappingError < ::Kafka::Error
end

class Kafka::InvalidReplicaAssignment < ::Kafka::ProtocolError
end

class Kafka::InvalidReplicationFactor < ::Kafka::ProtocolError
end

class Kafka::InvalidRequest < ::Kafka::ProtocolError
end

class Kafka::InvalidRequiredAcks < ::Kafka::ProtocolError
end

class Kafka::InvalidSaslState < ::Kafka::ProtocolError
end

class Kafka::InvalidSessionTimeout < ::Kafka::ProtocolError
end

class Kafka::InvalidTimestamp < ::Kafka::ProtocolError
end

class Kafka::InvalidTopic < ::Kafka::ProtocolError
end

class Kafka::InvalidTransactionTimeoutError < ::Kafka::Error
end

class Kafka::InvalidTxnStateError < ::Kafka::Error
end

class Kafka::LZ4Codec
  def codec_id; end
  def compress(data); end
  def decompress(data); end
  def load; end
  def produce_api_min_version; end
end

class Kafka::LeaderNotAvailable < ::Kafka::ProtocolError
end

class Kafka::MessageBuffer
  include(::Enumerable)

  def initialize; end

  def bytesize; end
  def clear; end
  def clear_messages(topic:, partition:); end
  def concat(messages, topic:, partition:); end
  def each; end
  def empty?; end
  def messages_for(topic:, partition:); end
  def size; end
  def to_h; end
  def write(value:, key:, topic:, partition:, create_time: T.unsafe(nil), headers: T.unsafe(nil)); end

  private

  def buffer_for(topic, partition); end
end

class Kafka::MessageSizeTooLarge < ::Kafka::ProtocolError
end

class Kafka::MessageTooLargeToRead < ::Kafka::Error
end

class Kafka::NetworkException < ::Kafka::ProtocolError
end

class Kafka::NoPartitionsToFetchFrom < ::Kafka::Error
end

class Kafka::NoSuchBroker < ::Kafka::Error
end

class Kafka::NotController < ::Kafka::ProtocolError
end

class Kafka::NotCoordinatorForGroup < ::Kafka::ProtocolError
end

class Kafka::NotEnoughReplicas < ::Kafka::ProtocolError
end

class Kafka::NotEnoughReplicasAfterAppend < ::Kafka::ProtocolError
end

class Kafka::NotLeaderForPartition < ::Kafka::ProtocolError
end

class Kafka::OffsetCommitError < ::Kafka::Error
end

class Kafka::OffsetManager
  def initialize(cluster:, group:, fetcher:, logger:, commit_interval:, commit_threshold:, offset_retention_time:); end

  def clear_offsets; end
  def clear_offsets_excluding(excluded); end
  def commit_offsets(recommit = T.unsafe(nil)); end
  def commit_offsets_if_necessary; end
  def mark_as_processed(topic, partition, offset); end
  def next_offset_for(topic, partition); end
  def seek_to(topic, partition, offset); end
  def seek_to_default(topic, partition); end
  def set_default_offset(topic, default_offset); end

  private

  def clear_resolved_offset(topic); end
  def commit_threshold_reached?; end
  def commit_timeout_reached?; end
  def committed_offset_for(topic, partition); end
  def committed_offsets; end
  def fetch_resolved_offsets(topic); end
  def offsets_to_commit(recommit = T.unsafe(nil)); end
  def offsets_to_recommit; end
  def prettify_offsets(offsets); end
  def recommit_timeout_reached?; end
  def resolve_offset(topic, partition); end
  def seconds_since(time); end
  def seconds_since_last_commit; end
end

Kafka::OffsetManager::DEFAULT_RETENTION_TIME = T.let(T.unsafe(nil), Integer)

class Kafka::OffsetMetadataTooLarge < ::Kafka::ProtocolError
end

class Kafka::OffsetOutOfRange < ::Kafka::ProtocolError
  def offset; end
  def offset=(_); end
  def partition; end
  def partition=(_); end
  def topic; end
  def topic=(_); end
end

class Kafka::OutOfOrderSequenceNumberError < ::Kafka::Error
end

class Kafka::Partitioner
  class << self
    def partition_for_key(partition_count, message); end
  end
end

class Kafka::Pause
  def initialize(clock: T.unsafe(nil)); end

  def expired?; end
  def pause!(timeout: T.unsafe(nil), max_timeout: T.unsafe(nil), exponential_backoff: T.unsafe(nil)); end
  def pause_duration; end
  def paused?; end
  def reset!; end
  def resume!; end

  private

  def ends_at; end
end

class Kafka::PendingMessage
  def initialize(value:, key:, topic:, partition:, partition_key:, create_time:, headers: T.unsafe(nil)); end

  def ==(other); end
  def bytesize; end
  def create_time; end
  def headers; end
  def key; end
  def partition; end
  def partition_key; end
  def topic; end
  def value; end
end

class Kafka::PendingMessageQueue
  def initialize; end

  def bytesize; end
  def clear; end
  def each(&block); end
  def empty?; end
  def replace(messages); end
  def size; end
  def write(message); end
end

class Kafka::PolicyViolation < ::Kafka::ProtocolError
end

class Kafka::ProcessingError < ::Kafka::Error
  def initialize(topic, partition, offset); end

  def offset; end
  def partition; end
  def topic; end
end

class Kafka::ProduceOperation
  def initialize(cluster:, transaction_manager:, buffer:, compressor:, required_acks:, ack_timeout:, logger:, instrumenter:); end

  def execute; end

  private

  def handle_response(broker, response, records_for_topics); end
  def send_buffered_messages; end
end

class Kafka::Producer
  def initialize(cluster:, transaction_manager:, logger:, instrumenter:, compressor:, ack_timeout:, required_acks:, max_retries:, retry_backoff:, max_buffer_size:, max_buffer_bytesize:); end

  def abort_transaction; end
  def begin_transaction; end
  def buffer_bytesize; end
  def buffer_size; end
  def clear_buffer; end
  def commit_transaction; end
  def deliver_messages; end
  def init_transactions; end
  def produce(value, topic:, key: T.unsafe(nil), headers: T.unsafe(nil), partition: T.unsafe(nil), partition_key: T.unsafe(nil), create_time: T.unsafe(nil)); end
  def send_offsets_to_transaction(batch:, group_id:); end
  def shutdown; end
  def to_s; end
  def transaction; end

  private

  def assign_partitions!; end
  def buffer_messages; end
  def buffer_overflow(topic, message); end
  def deliver_messages_with_retries(notification); end
  def pretty_partitions; end
end

class Kafka::Producer::AbortTransaction < ::StandardError
end

module Kafka::Protocol
  class << self
    def api_name(api_key); end
    def handle_error(error_code, error_message = T.unsafe(nil)); end
  end
end

Kafka::Protocol::ADD_OFFSETS_TO_TXN_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::ADD_PARTITIONS_TO_TXN_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::ALTER_CONFIGS_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::APIS = T.let(T.unsafe(nil), Hash)

Kafka::Protocol::API_VERSIONS_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::AddOffsetsToTxnRequest
  def initialize(producer_id:, producer_epoch:, group_id:, transactional_id: T.unsafe(nil)); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::AddOffsetsToTxnResponse
  def initialize(error_code:); end

  def error_code; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::AddPartitionsToTxnRequest
  def initialize(producer_id:, producer_epoch:, topics:, transactional_id: T.unsafe(nil)); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::AddPartitionsToTxnResponse
  def initialize(errors:); end

  def errors; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::AddPartitionsToTxnResponse::PartitionError
  def initialize(partition:, error_code:); end

  def error_code; end
  def partition; end
end

class Kafka::Protocol::AddPartitionsToTxnResponse::TopicPartitionsError
  def initialize(topic:, partitions:); end

  def partitions; end
  def topic; end
end

class Kafka::Protocol::AlterConfigsRequest
  def initialize(resources:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::AlterConfigsResponse
  def initialize(throttle_time_ms:, resources:); end

  def resources; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::AlterConfigsResponse::ResourceDescription
  def initialize(name:, type:, error_code:, error_message:); end

  def error_code; end
  def error_message; end
  def name; end
  def type; end
end

class Kafka::Protocol::ApiVersionsRequest
  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::ApiVersionsResponse
  def initialize(error_code:, apis:); end

  def apis; end
  def error_code; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::ApiVersionsResponse::ApiInfo
  def initialize(api_key:, min_version:, max_version:); end

  def api_key; end
  def api_name; end
  def inspect; end
  def max_version; end
  def min_version; end
  def to_s; end
  def version_supported?(version); end
end

Kafka::Protocol::COORDINATOR_TYPE_GROUP = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::COORDINATOR_TYPE_TRANSACTION = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::CREATE_PARTITIONS_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::CREATE_TOPICS_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::ConsumerGroupProtocol
  def initialize(topics:, version: T.unsafe(nil), user_data: T.unsafe(nil)); end

  def encode(encoder); end
end

class Kafka::Protocol::CreatePartitionsRequest
  def initialize(topics:, timeout:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::CreatePartitionsResponse
  def initialize(throttle_time_ms:, errors:); end

  def errors; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::CreateTopicsRequest
  def initialize(topics:, timeout:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::CreateTopicsResponse
  def initialize(errors:); end

  def errors; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::DELETE_TOPICS_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::DESCRIBE_CONFIGS_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::DESCRIBE_GROUPS_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::Decoder
  def initialize(io); end

  def array(&block); end
  def boolean; end
  def bytes; end
  def eof?; end
  def int16; end
  def int32; end
  def int64; end
  def int8; end
  def peek(offset, length); end
  def read(number_of_bytes); end
  def string; end
  def varint; end
  def varint_array(&block); end
  def varint_bytes; end
  def varint_string; end

  class << self
    def from_string(str); end
  end
end

class Kafka::Protocol::DeleteTopicsRequest
  def initialize(topics:, timeout:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::DeleteTopicsResponse
  def initialize(errors:); end

  def errors; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::DescribeConfigsRequest
  def initialize(resources:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::DescribeConfigsResponse
  def initialize(throttle_time_ms:, resources:); end

  def resources; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::DescribeConfigsResponse::ConfigEntry
  def initialize(name:, value:, read_only:, is_default:, is_sensitive:); end

  def is_default; end
  def is_sensitive; end
  def name; end
  def read_only; end
  def value; end
end

class Kafka::Protocol::DescribeConfigsResponse::ResourceDescription
  def initialize(name:, type:, error_code:, error_message:, configs:); end

  def configs; end
  def error_code; end
  def error_message; end
  def name; end
  def type; end
end

class Kafka::Protocol::DescribeGroupsRequest
  def initialize(group_ids:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::DescribeGroupsResponse
  def initialize(groups:); end

  def error_code; end
  def groups; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::DescribeGroupsResponse::Group
  def initialize(error_code:, group_id:, protocol_type:, protocol:, state:, members:); end

  def error_code; end
  def group_id; end
  def members; end
  def protocol; end
  def state; end
end

class Kafka::Protocol::DescribeGroupsResponse::Member
  def initialize(member_id:, client_id:, client_host:, member_assignment:); end

  def client_host; end
  def client_id; end
  def member_assignment; end
  def member_id; end
end

Kafka::Protocol::END_TXN_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::ERRORS = T.let(T.unsafe(nil), Hash)

class Kafka::Protocol::Encoder
  def initialize(io); end

  def write(bytes); end
  def write_array(array, &block); end
  def write_boolean(boolean); end
  def write_bytes(bytes); end
  def write_int16(int); end
  def write_int32(int); end
  def write_int64(int); end
  def write_int8(int); end
  def write_string(string); end
  def write_varint(int); end
  def write_varint_array(array, &block); end
  def write_varint_bytes(bytes); end
  def write_varint_string(string); end

  class << self
    def encode_with(object); end
  end
end

class Kafka::Protocol::EndTxnRequest
  def initialize(transactional_id:, producer_id:, producer_epoch:, transaction_result:); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::EndTxnResposne
  def initialize(error_code:); end

  def error_code; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::FETCH_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::FIND_COORDINATOR_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::FetchRequest
  def initialize(max_wait_time:, min_bytes:, max_bytes:, topics:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

Kafka::Protocol::FetchRequest::ISOLATION_READ_COMMITTED = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::FetchRequest::ISOLATION_READ_UNCOMMITTED = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::FetchResponse
  def initialize(topics: T.unsafe(nil), throttle_time_ms: T.unsafe(nil)); end

  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::FetchResponse::AbortedTransaction
  def initialize(producer_id:, first_offset:); end

  def first_offset; end
  def producer_id; end
end

class Kafka::Protocol::FetchResponse::FetchedPartition
  def initialize(partition:, error_code:, highwater_mark_offset:, last_stable_offset:, aborted_transactions:, messages:); end

  def aborted_transactions; end
  def error_code; end
  def highwater_mark_offset; end
  def last_stable_offset; end
  def messages; end
  def partition; end
end

class Kafka::Protocol::FetchResponse::FetchedTopic
  def initialize(name:, partitions:); end

  def name; end
  def partitions; end
end

Kafka::Protocol::FetchResponse::MAGIC_BYTE_LENGTH = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::FetchResponse::MAGIC_BYTE_OFFSET = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::FindCoordinatorRequest
  def initialize(coordinator_key:, coordinator_type:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::FindCoordinatorResponse
  def initialize(error_code:, error_message:, coordinator_id:, coordinator_host:, coordinator_port:); end

  def coordinator_host; end
  def coordinator_id; end
  def coordinator_port; end
  def error_code; end
  def error_message; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::HEARTBEAT_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::HeartbeatRequest
  def initialize(group_id:, generation_id:, member_id:); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::HeartbeatResponse
  def initialize(error_code:); end

  def error_code; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::INIT_PRODUCER_ID_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::InitProducerIDRequest
  def initialize(transactional_timeout:, transactional_id: T.unsafe(nil)); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::InitProducerIDResponse
  def initialize(error_code:, producer_id:, producer_epoch:); end

  def error_code; end
  def producer_epoch; end
  def producer_id; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::JOIN_GROUP_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::JoinGroupRequest
  def initialize(group_id:, session_timeout:, rebalance_timeout:, member_id:, topics: T.unsafe(nil)); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

Kafka::Protocol::JoinGroupRequest::PROTOCOL_TYPE = T.let(T.unsafe(nil), String)

class Kafka::Protocol::JoinGroupResponse
  def initialize(error_code:, generation_id:, group_protocol:, leader_id:, member_id:, members:); end

  def error_code; end
  def generation_id; end
  def group_protocol; end
  def leader_id; end
  def member_id; end
  def members; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::LEAVE_GROUP_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::LIST_GROUPS_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::LIST_OFFSET_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::LeaveGroupRequest
  def initialize(group_id:, member_id:); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::LeaveGroupResponse
  def initialize(error_code:); end

  def error_code; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::ListGroupsRequest
  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::ListGroupsResponse
  def initialize(error_code:, groups:); end

  def error_code; end
  def groups; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::ListGroupsResponse::GroupEntry
  def initialize(group_id:, protocol_type:); end

  def group_id; end
  def protocol_type; end
end

class Kafka::Protocol::ListOffsetRequest
  def initialize(topics:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

Kafka::Protocol::ListOffsetRequest::ISOLATION_READ_COMMITTED = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::ListOffsetRequest::ISOLATION_READ_UNCOMMITTED = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::ListOffsetResponse
  def initialize(topics:); end

  def offset_for(topic, partition); end
  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::ListOffsetResponse::PartitionOffsetInfo
  def initialize(partition:, error_code:, timestamp:, offset:); end

  def error_code; end
  def offset; end
  def partition; end
  def timestamp; end
end

class Kafka::Protocol::ListOffsetResponse::TopicOffsetInfo
  def initialize(name:, partition_offsets:); end

  def name; end
  def partition_offsets; end
end

class Kafka::Protocol::MemberAssignment
  def initialize(version: T.unsafe(nil), topics: T.unsafe(nil), user_data: T.unsafe(nil)); end

  def assign(topic, partitions); end
  def encode(encoder); end
  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::Message
  def initialize(value:, key: T.unsafe(nil), create_time: T.unsafe(nil), codec_id: T.unsafe(nil), offset: T.unsafe(nil)); end

  def ==(other); end
  def bytesize; end
  def codec_id; end
  def compressed?; end
  def create_time; end
  def decompress; end
  def encode(encoder); end
  def headers; end
  def is_control_record; end
  def key; end
  def offset; end
  def value; end

  private

  def correct_offsets(messages); end
  def encode_with_crc; end
  def encode_without_crc; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::Message::MAGIC_BYTE = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::MessageSet
  def initialize(messages: T.unsafe(nil)); end

  def ==(other); end
  def encode(encoder); end
  def messages; end
  def size; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::MetadataRequest
  def initialize(topics: T.unsafe(nil)); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::MetadataResponse
  def initialize(brokers:, controller_id:, topics:); end

  def brokers; end
  def controller_broker; end
  def controller_id; end
  def find_broker(node_id); end
  def find_leader_id(topic, partition); end
  def partitions_for(topic_name); end
  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::MetadataResponse::PartitionMetadata
  def initialize(partition_error_code:, partition_id:, leader:, replicas: T.unsafe(nil), isr: T.unsafe(nil)); end

  def leader; end
  def partition_error_code; end
  def partition_id; end
  def replicas; end
end

class Kafka::Protocol::MetadataResponse::TopicMetadata
  def initialize(topic_name:, partitions:, topic_error_code: T.unsafe(nil)); end

  def partitions; end
  def topic_error_code; end
  def topic_name; end
end

Kafka::Protocol::OFFSET_COMMIT_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::OFFSET_FETCH_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::OffsetCommitRequest
  def initialize(group_id:, generation_id:, member_id:, offsets:, retention_time: T.unsafe(nil)); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

Kafka::Protocol::OffsetCommitRequest::DEFAULT_RETENTION_TIME = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::OffsetCommitResponse
  def initialize(topics:); end

  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::OffsetFetchRequest
  def initialize(group_id:, topics:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::OffsetFetchResponse
  def initialize(topics:); end

  def offset_for(topic, partition); end
  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::OffsetFetchResponse::PartitionOffsetInfo
  def initialize(offset:, metadata:, error_code:); end

  def error_code; end
  def metadata; end
  def offset; end
end

Kafka::Protocol::PRODUCE_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::ProduceRequest
  def initialize(required_acks:, timeout:, messages_for_topics:, transactional_id: T.unsafe(nil), compressor: T.unsafe(nil)); end

  def api_key; end
  def api_version; end
  def compressor; end
  def encode(encoder); end
  def messages_for_topics; end
  def required_acks; end
  def requires_acks?; end
  def response_class; end
  def timeout; end
  def transactional_id; end

  private

  def compress(record_batch); end
end

Kafka::Protocol::ProduceRequest::API_MIN_VERSION = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::ProduceResponse
  def initialize(topics: T.unsafe(nil), throttle_time_ms: T.unsafe(nil)); end

  def each_partition; end
  def throttle_time_ms; end
  def topics; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::ProduceResponse::PartitionInfo
  def initialize(partition:, error_code:, offset:, timestamp:); end

  def error_code; end
  def offset; end
  def partition; end
  def timestamp; end
end

class Kafka::Protocol::ProduceResponse::TopicInfo
  def initialize(topic:, partitions:); end

  def partitions; end
  def topic; end
end

Kafka::Protocol::REPLICA_ID = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPES = T.let(T.unsafe(nil), Hash)

Kafka::Protocol::RESOURCE_TYPE_ANY = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPE_CLUSTER = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPE_DELEGATION_TOKEN = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPE_GROUP = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPE_TOPIC = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPE_TRANSACTIONAL_ID = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RESOURCE_TYPE_UNKNOWN = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::Record
  def initialize(value:, key: T.unsafe(nil), headers: T.unsafe(nil), attributes: T.unsafe(nil), offset_delta: T.unsafe(nil), offset: T.unsafe(nil), timestamp_delta: T.unsafe(nil), create_time: T.unsafe(nil), is_control_record: T.unsafe(nil)); end

  def ==(other); end
  def attributes; end
  def bytesize; end
  def create_time; end
  def create_time=(_); end
  def encode(encoder); end
  def headers; end
  def is_control_record; end
  def is_control_record=(_); end
  def key; end
  def offset; end
  def offset=(_); end
  def offset_delta; end
  def offset_delta=(_); end
  def timestamp_delta; end
  def timestamp_delta=(_); end
  def value; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::RecordBatch
  def initialize(records: T.unsafe(nil), first_offset: T.unsafe(nil), first_timestamp: T.unsafe(nil), partition_leader_epoch: T.unsafe(nil), codec_id: T.unsafe(nil), in_transaction: T.unsafe(nil), is_control_batch: T.unsafe(nil), last_offset_delta: T.unsafe(nil), producer_id: T.unsafe(nil), producer_epoch: T.unsafe(nil), first_sequence: T.unsafe(nil), max_timestamp: T.unsafe(nil)); end

  def ==(other); end
  def attributes; end
  def codec_id; end
  def codec_id=(_); end
  def compressed?; end
  def encode(encoder); end
  def encode_record_array; end
  def encode_record_batch_body; end
  def first_offset; end
  def first_sequence; end
  def first_timestamp; end
  def fulfill_relative_data; end
  def in_transaction; end
  def is_control_batch; end
  def last_offset; end
  def last_offset_delta; end
  def mark_control_record; end
  def max_timestamp; end
  def partition_leader_epoch; end
  def producer_epoch; end
  def producer_id; end
  def records; end
  def size; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::RecordBatch::CODEC_ID_MASK = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RecordBatch::IN_TRANSACTION_MASK = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RecordBatch::IS_CONTROL_BATCH_MASK = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RecordBatch::MAGIC_BYTE = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RecordBatch::RECORD_BATCH_OVERHEAD = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::RecordBatch::TIMESTAMP_TYPE_MASK = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::RequestMessage
  def initialize(api_key:, correlation_id:, client_id:, request:, api_version: T.unsafe(nil)); end

  def encode(encoder); end
end

Kafka::Protocol::RequestMessage::API_VERSION = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::SASL_HANDSHAKE_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::SYNC_GROUP_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::SaslHandshakeRequest
  def initialize(mechanism); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

Kafka::Protocol::SaslHandshakeRequest::SUPPORTED_MECHANISMS = T.let(T.unsafe(nil), Array)

class Kafka::Protocol::SaslHandshakeResponse
  def initialize(error_code:, enabled_mechanisms:); end

  def enabled_mechanisms; end
  def error_code; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::Protocol::SyncGroupRequest
  def initialize(group_id:, generation_id:, member_id:, group_assignment: T.unsafe(nil)); end

  def api_key; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::SyncGroupResponse
  def initialize(error_code:, member_assignment:); end

  def error_code; end
  def member_assignment; end

  class << self
    def decode(decoder); end
  end
end

Kafka::Protocol::TOPIC_METADATA_API = T.let(T.unsafe(nil), Integer)

Kafka::Protocol::TXN_OFFSET_COMMIT_API = T.let(T.unsafe(nil), Integer)

class Kafka::Protocol::TxnOffsetCommitRequest
  def initialize(transactional_id:, group_id:, producer_id:, producer_epoch:, offsets:); end

  def api_key; end
  def api_version; end
  def encode(encoder); end
  def response_class; end
end

class Kafka::Protocol::TxnOffsetCommitResponse
  def initialize(error_code:); end

  def error_code; end

  class << self
    def decode(decoder); end
  end
end

class Kafka::ProtocolError < ::Kafka::Error
end

class Kafka::RebalanceInProgress < ::Kafka::ProtocolError
end

class Kafka::RecordListTooLarge < ::Kafka::ProtocolError
end

class Kafka::ReplicaNotAvailable < ::Kafka::ProtocolError
end

class Kafka::RequestTimedOut < ::Kafka::ProtocolError
end

class Kafka::RoundRobinAssignmentStrategy
  def initialize(cluster:); end

  def assign(members:, topics:); end
end

class Kafka::SSLSocketWithTimeout
  def initialize(host, port, ssl_context:, connect_timeout: T.unsafe(nil), timeout: T.unsafe(nil)); end

  def close; end
  def closed?; end
  def read(num_bytes); end
  def select_with_timeout(socket, type); end
  def set_encoding(encoding); end
  def write(bytes); end
end

module Kafka::Sasl
end

class Kafka::Sasl::Gssapi
  def initialize(logger:, principal:, keytab:); end

  def authenticate!(host, encoder, decoder); end
  def configured?; end
  def handshake_messages; end
  def ident; end
  def initialize_gssapi_context(host); end
  def load_gssapi; end
  def send_and_receive_sasl_token; end
end

Kafka::Sasl::Gssapi::GSSAPI_IDENT = T.let(T.unsafe(nil), String)

class Kafka::Sasl::OAuth
  def initialize(logger:, token_provider:); end

  def authenticate!(host, encoder, decoder); end
  def configured?; end
  def ident; end

  private

  def initial_client_response; end
  def token_extensions; end
end

Kafka::Sasl::OAuth::OAUTH_IDENT = T.let(T.unsafe(nil), String)

class Kafka::Sasl::Plain
  def initialize(logger:, authzid:, username:, password:); end

  def authenticate!(host, encoder, decoder); end
  def configured?; end
  def ident; end
end

Kafka::Sasl::Plain::PLAIN_IDENT = T.let(T.unsafe(nil), String)

class Kafka::Sasl::Scram
  def initialize(username:, password:, logger:, mechanism: T.unsafe(nil)); end

  def authenticate!(host, encoder, decoder); end
  def configured?; end
  def ident; end

  private

  def auth_message; end
  def client_key; end
  def client_proof; end
  def client_signature; end
  def digest; end
  def encoded_username; end
  def final_message; end
  def final_message_without_proof; end
  def first_message; end
  def first_message_bare; end
  def h(str); end
  def hi(str, salt, iterations); end
  def hmac(data, key); end
  def iterations; end
  def nonce; end
  def parse_response(data); end
  def rnonce; end
  def safe_str(val); end
  def salt; end
  def salted_password; end
  def server_data; end
  def server_key; end
  def server_signature; end
  def stored_key; end
  def xor(first, second); end
end

Kafka::Sasl::Scram::MECHANISMS = T.let(T.unsafe(nil), Hash)

class Kafka::SaslAuthenticator
  def initialize(logger:, sasl_gssapi_principal:, sasl_gssapi_keytab:, sasl_plain_authzid:, sasl_plain_username:, sasl_plain_password:, sasl_scram_username:, sasl_scram_password:, sasl_scram_mechanism:, sasl_oauth_token_provider:); end

  def authenticate!(connection); end
  def enabled?; end
end

class Kafka::SaslScramError < ::Kafka::Error
end

class Kafka::SnappyCodec
  def codec_id; end
  def compress(data); end
  def decompress(data); end
  def load; end
  def produce_api_min_version; end
end

class Kafka::SocketWithTimeout
  def initialize(host, port, connect_timeout: T.unsafe(nil), timeout: T.unsafe(nil)); end

  def close; end
  def closed?; end
  def read(num_bytes); end
  def set_encoding(encoding); end
  def write(bytes); end
end

module Kafka::SslContext
  class << self
    def build(ca_cert_file_path: T.unsafe(nil), ca_cert: T.unsafe(nil), client_cert: T.unsafe(nil), client_cert_key: T.unsafe(nil), client_cert_key_password: T.unsafe(nil), client_cert_chain: T.unsafe(nil), ca_certs_from_system: T.unsafe(nil), verify_hostname: T.unsafe(nil)); end
  end
end

Kafka::SslContext::CLIENT_CERT_DELIMITER = T.let(T.unsafe(nil), String)

class Kafka::StaleControllerEpoch < ::Kafka::ProtocolError
end

class Kafka::TaggedLogger < ::SimpleDelegator
  def initialize(logger_or_stream = T.unsafe(nil)); end

  def clear_tags!; end
  def current_tags; end
  def debug(msg_or_progname, &block); end
  def error(msg_or_progname, &block); end
  def flush; end
  def info(msg_or_progname, &block); end
  def pop_tags(size = T.unsafe(nil)); end
  def push_tags(*tags); end
  def tagged(*tags); end
  def tags_text; end
  def warn(msg_or_progname, &block); end

  class << self
    def new(logger_or_stream = T.unsafe(nil)); end
  end
end

class Kafka::TokenMethodNotImplementedError < ::Kafka::Error
end

class Kafka::TopicAlreadyExists < ::Kafka::ProtocolError
end

class Kafka::TopicAuthorizationFailed < ::Kafka::ProtocolError
end

class Kafka::TransactionCoordinatorFencedError < ::Kafka::Error
end

class Kafka::TransactionManager
  def initialize(cluster:, logger:, idempotent: T.unsafe(nil), transactional: T.unsafe(nil), transactional_id: T.unsafe(nil), transactional_timeout: T.unsafe(nil)); end

  def abort_transaction; end
  def add_partitions_to_transaction(topic_partitions); end
  def begin_transaction; end
  def close; end
  def commit_transaction; end
  def error?; end
  def idempotent?; end
  def in_transaction?; end
  def init_producer_id(force = T.unsafe(nil)); end
  def init_transactions; end
  def next_sequence_for(topic, partition); end
  def producer_epoch; end
  def producer_id; end
  def send_offsets_to_txn(offsets:, group_id:); end
  def transactional?; end
  def transactional_id; end
  def update_sequence_for(topic, partition, sequence); end

  private

  def complete_transaction; end
  def force_transactional!; end
  def transaction_coordinator; end
end

Kafka::TransactionManager::DEFAULT_TRANSACTION_TIMEOUT = T.let(T.unsafe(nil), Integer)

Kafka::TransactionManager::TRANSACTION_RESULT_COMMIT = T.let(T.unsafe(nil), TrueClass)

class Kafka::TransactionStateMachine
  def initialize(logger:); end

  def aborting_transaction?; end
  def committing_transaction?; end
  def error?; end
  def in_transaction?; end
  def ready?; end
  def transition_to!(next_state); end
  def uninitialized?; end

  private

  def in_state?(state); end
end

Kafka::TransactionStateMachine::ABORTING_TRANSACTION = T.let(T.unsafe(nil), Symbol)

Kafka::TransactionStateMachine::COMMITTING_TRANSACTION = T.let(T.unsafe(nil), Symbol)

Kafka::TransactionStateMachine::ERROR = T.let(T.unsafe(nil), Symbol)

Kafka::TransactionStateMachine::IN_TRANSACTION = T.let(T.unsafe(nil), Symbol)

class Kafka::TransactionStateMachine::InvalidStateError < ::StandardError
end

class Kafka::TransactionStateMachine::InvalidTransitionError < ::StandardError
end

Kafka::TransactionStateMachine::READY = T.let(T.unsafe(nil), Symbol)

Kafka::TransactionStateMachine::STATES = T.let(T.unsafe(nil), Array)

Kafka::TransactionStateMachine::TRANSITIONS = T.let(T.unsafe(nil), Hash)

Kafka::TransactionStateMachine::UNINITIALIZED = T.let(T.unsafe(nil), Symbol)

class Kafka::UnknownError < ::Kafka::ProtocolError
end

class Kafka::UnknownMemberId < ::Kafka::ProtocolError
end

class Kafka::UnknownTopicOrPartition < ::Kafka::ProtocolError
end

class Kafka::UnsupportedForMessageFormat < ::Kafka::ProtocolError
end

class Kafka::UnsupportedSaslMechanism < ::Kafka::ProtocolError
end

class Kafka::UnsupportedVersion < ::Kafka::ProtocolError
end

Kafka::VERSION = T.let(T.unsafe(nil), String)

class Kafka::ZstdCodec
  def codec_id; end
  def compress(data); end
  def decompress(data); end
  def load; end
  def produce_api_min_version; end
end
