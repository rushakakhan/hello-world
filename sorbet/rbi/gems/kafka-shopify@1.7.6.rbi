# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `kafka-shopify` gem.
# Please instead update this file by running `tapioca generate`.

# typed: true

module KafkaShopify
  extend(::KafkaShopify)

  def add_error_handler(&handler); end
  def add_interceptor(interceptor); end
  def app_name; end
  def app_name=(_); end
  def close; end
  def context; end
  def eager_load!; end
  def enable_statsd!(sample_rate: T.unsafe(nil), global_tags: T.unsafe(nil)); end
  def error_handlers; end
  def inner_produce_with(producer, topic_name, event_name, headers, attributes = T.unsafe(nil)); end
  def interceptors; end
  def log_contexts; end
  def logger; end
  def logger=(_); end
  def produce(topic_name, event_name, attributes = T.unsafe(nil)); end
  def produce_with(producer, topic_name, event_name, attributes = T.unsafe(nil)); end
  def producer; end
  def producer=(_); end
  def schemas_path; end
  def schemas_path=(_); end
  def statsd; end
  def statsd=(val); end
  def topics; end
  def topics_file; end
  def topics_file=(filename); end

  private

  def load_topic_from_file(path); end
end

module KafkaShopify::ActiveRecord
  extend(::ActiveSupport::Concern)

  mixes_in_class_methods(::KafkaShopify::ActiveRecord::ClassMethods)

  def kafka_event(topic, name, message = T.unsafe(nil)); end
  def kafka_event!(topic, name, message = T.unsafe(nil)); end

  protected

  def event_context; end
  def flush_kafka_deferred_events; end
  def kafka_deferred_events; end
  def kafka_event_attributes_changed?; end
  def kafka_event_attributes_saved_changes?; end
  def kafka_staged_events; end
  def reset_kafka_deferred_events; end
  def reset_kafka_staged_events; end
  def rollback_kafka_deferred_events; end
  def stage_kafka_deferred_events; end

  private

  def statsd_options; end
end

module KafkaShopify::ActiveRecord::ClassMethods
  def kafka_event_attributes(*methods); end
end

module KafkaShopify::Environment
  class << self
    def default_producer; end
    def development?; end
    def env; end
    def internal?; end
    def production?; end
    def test?; end
  end
end

class KafkaShopify::Error < ::StandardError
end

module KafkaShopify::EventAssertions
  def assert_kafka_event(topic = T.unsafe(nil), name = T.unsafe(nil), expectations = T.unsafe(nil), expected_number = T.unsafe(nil), &code); end
  def assert_no_kafka_event(topic = T.unsafe(nil), name = T.unsafe(nil), expectations = T.unsafe(nil), &code); end
  def assert_valid_kafka_event(topic, event); end
  def capture_kafka_events; end
  def filter_kafka_events(events, expectations); end

  private

  def dump_events(events, message: T.unsafe(nil)); end
  def filter_event_parts(event, expectations); end
end

KafkaShopify::EventAssertions::ISO8601_UTC_TIMESTAMP = T.let(T.unsafe(nil), Regexp)

class KafkaShopify::EventOnUnsavedModel < ::KafkaShopify::Error
end

class KafkaShopify::EventProductionFailure < ::KafkaShopify::Error
end

module KafkaShopify::Failure
end

class KafkaShopify::Failure::Producer < ::KafkaShopify::Producer
  def produce(_topic, _key, _payload, _options = T.unsafe(nil)); end
end

class KafkaShopify::InvalidConfiguration < ::KafkaShopify::Error
end

class KafkaShopify::InvalidProducer < ::KafkaShopify::Error
end

module KafkaShopify::KafkaClientRuby
  class << self
    def can_autodetect?; end
  end
end

class KafkaShopify::KafkaClientRuby::Producer < ::KafkaShopify::Producer
  def initialize(retry_on_failure: T.unsafe(nil)); end

  def close; end
  def partitions_for(topic); end
  def produce(topic, key, payload, options = T.unsafe(nil)); end

  private

  def produce_with_retry(topic, key, payload, options = T.unsafe(nil)); end
  def produce_without_retry(topic, key, payload, options = T.unsafe(nil)); end
end

KafkaShopify::KafkaClientRuby::Producer::MAX_RETRIES = T.let(T.unsafe(nil), Integer)

KafkaShopify::LOG_THREAD_VARIABLE_NAME = T.let(T.unsafe(nil), String)

module KafkaShopify::Logger
  class << self
    def stdout_logger; end
  end
end

class KafkaShopify::Logger::Producer < ::KafkaShopify::Producer
  def initialize(logger = T.unsafe(nil)); end

  def logger; end
  def logger=(_); end
  def partitions_for(topic); end
  def produce(topic, key, payload, options = T.unsafe(nil)); end
end

KafkaShopify::MAX_MESSAGE_SIZE = T.let(T.unsafe(nil), Integer)

module KafkaShopify::Mock
end

class KafkaShopify::Mock::Producer < ::KafkaShopify::Producer
  def initialize; end

  def events(topic: T.unsafe(nil), with_metadata: T.unsafe(nil)); end
  def flush(topic: T.unsafe(nil)); end
  def partitions_for(_topic); end
  def produce(topic, key, payload, options = T.unsafe(nil)); end
  def produce_with_headers(topic, key, payload, headers, _options = T.unsafe(nil)); end
end

class KafkaShopify::NoTopicError < ::KafkaShopify::Error
end

class KafkaShopify::NonExistingSchema < ::KafkaShopify::EventProductionFailure
end

class KafkaShopify::NonExistingTopic < ::KafkaShopify::EventProductionFailure
end

class KafkaShopify::OffsetSpecifiedWithoutPartition < ::KafkaShopify::Error
end

class KafkaShopify::ProduceQuotaExceeded < ::KafkaShopify::Error
end

class KafkaShopify::Producer
  def close; end
  def default_event_attributes(topic, name); end
  def flatten_hash(hash, keys = T.unsafe(nil)); end
  def hostname; end
  def next_serial_id(topic); end
  def partitions_for(_topic); end
  def prepare_event(topic_name, event_name, event, json_encoder: T.unsafe(nil)); end
  def produce(_topic, _key, _payload, _options = T.unsafe(nil)); end
  def produce_with_headers(topic, key, payload, _headers, options = T.unsafe(nil)); end
  def reset_serial_group(topic, pid = T.unsafe(nil)); end
  def reset_serials!; end
  def serial_group(topic, pid = T.unsafe(nil)); end
  def serial_groups; end
  def serial_ids; end
  def type; end
  def validation_error(warning, topic, event); end
end

KafkaShopify::Producer::DEFAULT_EVENT_ATTRIBUTE_NAMES = T.let(T.unsafe(nil), Array)

KafkaShopify::Producer::MAX_MESSAGE_SIZE = T.let(T.unsafe(nil), Integer)

class KafkaShopify::Railtie < ::Rails::Railtie
end

module KafkaShopify::RubyKafka
end

class KafkaShopify::RubyKafka::Producer < ::KafkaShopify::Producer
  def initialize(brokers: T.unsafe(nil), ssl_client_cert: T.unsafe(nil), ssl_client_cert_key: T.unsafe(nil), ssl_ca_cert: T.unsafe(nil), retry_on_failure: T.unsafe(nil)); end

  def close; end
  def partitions_for(topic); end
  def produce(topic, key, payload, options = T.unsafe(nil)); end

  private

  def produce_with_retry(topic, key, payload, options = T.unsafe(nil)); end
  def produce_without_retry(topic, key, payload, options = T.unsafe(nil)); end
  def shitlist; end
end

KafkaShopify::RubyKafka::Producer::MAX_RETRIES = T.let(T.unsafe(nil), Integer)

class KafkaShopify::SchemaValidationFailed < ::KafkaShopify::EventProductionFailure
  def initialize(message, topic, event); end

  def event; end
  def topic; end
end

class KafkaShopify::StatsDConfig
  def initialize(enabled: T.unsafe(nil), sample_rate: T.unsafe(nil), default_tags: T.unsafe(nil)); end

  def default_tags; end
  def default_tags=(_); end
  def enabled; end
  def enabled=(_); end
  def enabled?; end
  def options(tags = T.unsafe(nil)); end
  def sample_rate; end
  def sample_rate=(_); end
end

module KafkaShopify::Sysv
  class << self
    def encode_v2_event(topic, key, payload); end
    def encode_v3_event(topic, key, payload); end
    def encode_v4_event(topic, key, headers, payload); end
    def maximum_size(key = T.unsafe(nil)); end
    def message_queue_exists?(key = T.unsafe(nil)); end
  end
end

KafkaShopify::Sysv::DEFAULT_QUEUE_KEY = T.let(T.unsafe(nil), Integer)

KafkaShopify::Sysv::DEFAULT_QUEUE_MODE = T.let(T.unsafe(nil), Integer)

class KafkaShopify::Sysv::Producer < ::KafkaShopify::Producer
  def initialize(queue_key: T.unsafe(nil), message_size: T.unsafe(nil), queue_mode: T.unsafe(nil)); end

  def produce(topic, key, payload, options = T.unsafe(nil)); end
  def produce_with_headers(topic, key, payload, headers, _options = T.unsafe(nil)); end
  def queue; end
end

KafkaShopify::Sysv::Producer::MESSAGE_TYPE = T.let(T.unsafe(nil), Integer)

KafkaShopify::THREAD_VARIABLE_NAME = T.let(T.unsafe(nil), String)

class KafkaShopify::Topic
  def initialize(name, key_field: T.unsafe(nil), schema: T.unsafe(nil), allow_empty_key: T.unsafe(nil), allow_invalid_events: T.unsafe(nil), raise_on_failure: T.unsafe(nil)); end

  def allow_empty_key?; end
  def allow_invalid_events?; end
  def eql?(other); end
  def extract_partition_key_from_event(event); end
  def has_schema?; end
  def hash; end
  def key_field; end
  def name; end
  def partitioned_by_key?; end
  def raise_on_failure?; end
  def schema; end
  def schema_name; end
  def to_s; end
  def valid_record?(record); end
end

class Tros::Schema
  def initialize(type); end

  def ==(other, seen = T.unsafe(nil)); end
  def hash(seen = T.unsafe(nil)); end
  def subparse(json_obj, names = T.unsafe(nil), namespace = T.unsafe(nil)); end
  def to_avro(names = T.unsafe(nil)); end
  def to_s; end
  def type; end
  def type_sym; end

  class << self
    def parse(json_string); end
    def real_parse(json_obj, names = T.unsafe(nil), default_namespace = T.unsafe(nil)); end
    def validate(expected_schema, datum); end
    def validate_strictly(expected_schema, datum, validator_method = T.unsafe(nil)); end
    def validate_with_date_handling(expected_schema, datum); end
    def validate_without_date_handling(expected_schema, datum, validator_method = T.unsafe(nil)); end
  end
end

Tros::Schema::INT_MAX_VALUE = T.let(T.unsafe(nil), Integer)

Tros::Schema::INT_MIN_VALUE = T.let(T.unsafe(nil), Integer)

Tros::Schema::LONG_MAX_VALUE = T.let(T.unsafe(nil), Integer)

Tros::Schema::LONG_MIN_VALUE = T.let(T.unsafe(nil), Integer)

Tros::Schema::NAMED_TYPES = T.let(T.unsafe(nil), Set)

Tros::Schema::NAMED_TYPES_SYM = T.let(T.unsafe(nil), Set)

Tros::Schema::PRIMITIVE_TYPES = T.let(T.unsafe(nil), Set)

Tros::Schema::PRIMITIVE_TYPES_SYM = T.let(T.unsafe(nil), Set)

Tros::Schema::VALID_TYPES = T.let(T.unsafe(nil), Set)

Tros::Schema::VALID_TYPES_SYM = T.let(T.unsafe(nil), Set)
